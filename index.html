<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIXAI</title>
    <link rel="stylesheet" href="styles.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Josefin+Sans:ital,wght@0,100..700;1,100..700&display=swap"
        rel="stylesheet">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300..700&display=swap" rel="stylesheet">

    <script type="module" src="https://ajax.googleapis.com/ajax/libs/model-viewer/3.5.0/model-viewer.min.js"></script>

</head>

<body>
    <header>
        <a href="#" class="logo">
            <span class="logo-text">AIxAI</span>
            <span class="logo-hover">Artificial Intelligence x Architectural Intelligence</span>
        </a>

        <nav>

            <div class="nav-menu">
                <a href="#Intro">Introduction</a>
                <a href="#H-bloc">Homunculus' bloc</a>
                <a href="#Voxel">Voxel System</a>
                <a href="#GH">Grasshopper Plugin</a>
                <a href="#about">About</a>
                <a href="#contact">Contact</a>
            </div>
        </nav>
    </header>

    <div class="container">
        <!-- 
        //    _                                                          _               
        //   | |                                                        | |              
        //   | |__     ___    _ __ ___    _   _   _ __     ___   _   _  | |  _   _   ___ 
        //   | '_ \   / _ \  | '_ ` _ \  | | | | | '_ \   / __| | | | | | | | | | | / __|
        //   | | | | | (_) | | | | | | | | |_| | | | | | | (__  | |_| | | | | |_| | \__ \
        //   |_| |_|  \___/  |_| |_| |_|  \__,_| |_| |_|  \___|  \__,_| |_|  \__,_| |___/
        //                                                                               
        //                  
        -->
        <section id="Intro" class="Intro">
            <h1>INTRODUCTION</h1>
            <div>
                
                <iframe width="560" height="315" src="https://www.youtube.com/embed/xL737XE9c8c?si=8vzm6a2egrSCxYTD" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <p class = footnote>H-bloc Demonstration</p>
            </div>
            <div>
                
                <iframe width="560" height="315" src="https://www.youtube.com/embed/JGj_xInnKJw?si=gytWURGz6Cuq4tMG" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <p class = footnote>H-bloc Technologies Stack</p>
            </div>
            <div class="H-bloc-item">
                <h2>AIxAI</h2>

                <p>
                    AIxAI (Artificial Intelligence x Architectural Intelligence) is a project designed to develop a
                    workflow that employs machine learning to enable a computer to monitor physical 3D-printed blocks.
                    This system facilitates multiple methods of hands-on involvement in the design process. Architects
                    can intuitively manipulate the blocks, and the system will convert those physical attributes into
                    digital models. This approach preserves the natural creativity of manual design while utilizing
                    computational power.</p>

                <p>
                    Computer-assisted design technologies are now so dominant that they greatly reduce the need for
                    designers' hands-on involvement in the design process. Architects use a variety of advanced digital
                    approaches (BIM, parametrical modeling, Procedural modeling, generative AI) to solve design
                    problems. But does this evolution enhance architectural intelligence? Do we still need hands-on
                    involvement to improve architectural intelligence?</p>




                <img src="assets/Homonculus.png" alt="Homonculus">
                <p class="Intro-item">The "sensory homunculus" or "motor homunculus" is a visual representation of the
                    human body that illustrates how the brain perceives different parts of the body in terms of sensory
                    input or motor control. In the sensory homunculus, body parts are depicted in proportion to how much
                    sensory cortex is dedicated to them. For example, the hands and lips are shown much larger because
                    they have more sensory nerve endings and thus more brain power dedicated to them. The same applies
                    to the motor homunculus, which represents the brain's control over body movements.</p>



                <img src="assets/hOMUNCULUS.png" alt="Homonculus">
                <p class=footnote>Sensory homunculus model by Sharon Price-James. The cortical homunculus represents the
                    area of the
                    brain's cortex dedicated to sensory functions.</p>

                <p>The rise of computers, smartphones, and touch screens has significantly decreased the practice of
                    real-world, hands-on skills. Nowadays, we do almost everything virtually, using computer screens. We
                    primarily use our hands to tap keys and swipe screens, which has somewhat reduced the stimulation
                    received by the sensory motor cortex. If we are not fully utilizing our hands, perhaps we are also
                    not fully utilizing our brains. What might be the long-term effects of this on us and our society?
                    As the old saying goes, "Use it or lose it." Only time will tell.</p>




        </section>

        <!-- 
//    _                                                          _                   _       _                
//   | |                                                        | |                 | |     | |               
//   | |__     ___    _ __ ___    _   _   _ __     ___   _   _  | |  _   _   ___    | |__   | |   ___     ___ 
//   | '_ \   / _ \  | '_ ` _ \  | | | | | '_ \   / __| | | | | | | | | | | / __|   | '_ \  | |  / _ \   / __|
//   | | | | | (_) | | | | | | | | |_| | | | | | | (__  | |_| | | | | |_| | \__ \   | |_) | | | | (_) | | (__ 
//   |_| |_|  \___/  |_| |_| |_|  \__,_| |_| |_|  \___|  \__,_| |_|  \__,_| |___/   |_.__/  |_|  \___/   \___|
//                                                                                                            
-->

        <section id="H-bloc" class="H-bloc">
            <h1>Homunculus' Bloc</h1>

            <p>
                The Homunculus Block is a 3D-printable object that can interact with human hands and be interlockable
                with other blocks. It is also designed to allow a computer to efficiently track their unique IDs,
                positions, and orientations. These attributes are synchronized with the digital environment, so the
                computer is aware of hand interactions in real-time.
            </p>

            <div>
                <!-- Import the component -->
                <!-- <model-viewer camera-controls touch-action="pan-y" disable-zoom src="assets/bloc1.glb" alt="A 3D model of an astronaut"></model-viewer> -->

                <model-viewer camera-controls touch-action="pan-y" disable-zoom src="assets/bloc all.glb"
                    alt="A 3D model of an astronaut"></model-viewer>

            </div>
            <div class="H-bloc-item">

            </div>

            <div class="H-bloc-item">
                <h2>Computer Vision</h2>
                <p>Synthetic data that generated by computers is cost-effective for machine learning and computer vision
                    processes. The trained model enables the computer to recognize the target object in the physical
                    world.</p>
            </div>


            <div>
                <video src="assets/P-Block.mp4" controls autoplay loop muted onmouseover="this.controls=false"></video>
                <p class=footnote>An illustration of the machine learning process.</p>

            </div>


            <div>
                <p>The video below demonstrates the results achieved by this technology. The digital modeling
                    environment captures attributes from the blocks and then generates input for the diffusion model to
                    create images from it.</p>

                <iframe
                    src="https://www.youtube.com/embed/L1v9sZLn3c8?si=nufwl8QdIUbZ-2F3&amp;controls=0&autoplay=1&loop=1"
                    frameborder="0" allow="autoplay; encrypted-media" referrerpolicy="strict-origin-when-cross-origin">
                </iframe>
                <p class=footnote>An Illustration of the training results.</p>
            </div>

        </section>



        <section id="Voxel" class="Voxel">
            <h1>Voxel System</h1>
            <p>I found that if I can add a sementic color-coded system to the traditional Wave Function Collapse (WFC)
                method, it allows me to interactively control the results of WFC, potentially turning it into an
                interactive design tool. The key is to create a 3D voxel meta-structure that stores semantic attributes
                using color, value, and tensor information of the voxel. The algorithm then link the meta-tile to
                detailed tiles in real-time.</p>

            <div class="Voxel-item">
                <image src="assets/image.png" style="width: 706px" alt="Homonculus">
                    <p class="footnote"> Color Voxel System </p>
                    <img src="assets\PH3.png" style="width: 706px" alt="Project 2">
                    <p class="footnote"> Color Voxel System in 3D View </p>

                    <p>In 3D Wave Function Collapse (WFC), a voxel has 26 possible neighboring positions, leading to
                        2^26 potential neighbor configurations, which is an extremely large number. To manage this
                        complexity, consider the moon's phases as an analogy: just as the elliptical phase is the most
                        common moon phase, most voxel neighbors have common patterns. Similar to the regularity of a
                        full moon occurring every month, some voxel neighbor configurations are less frequent but still
                        predictable. Additionally, there are rare events like the supermoon, blue moon, and harvest
                        moon, which represent the least common voxel neighbor configurations.</p>
                    <image class="no-scale" src="assets/26Neighbors.png" style="width: 706px" alt="Homonculus">
                        <p class="footnote"> The 26 neighbors of a voxel </p>

                        <img src="assets\Changes.png" style=" max-width: 300px" alt="moons">

                        <p class="footnote"> Possibilities of neighbor-conditions </p>

                        <img src="assets\Moons.jpg" style=" max-width: 602px" alt="moons">

                        <p class="footnote"> The 26 neighbors of a voxel </p>


                        <p>The video below shows the voxel sculpting tool (WIP), allowing designers to intuitively
                            control the voxel appearance with multiple sets of semantic WFC modules.</p>

                        <iframe src="https://www.youtube.com/embed/H6aqc9BjUYY?si=B-xdrZvG2pJ4C8G5&autoplay=1&loop=1"
                            frameborder="0" allow="autoplay; encrypted-media"
                            referrerpolicy="strict-origin-when-cross-origin">
                        </iframe>



                        <h3>Renderings</h3>
            </div>
            <div class="Voxel-item">
                <img src="assets\PH2.png" alt="Project 2">
                <p class="footnote"> This is a placeholder rendering</p>

            </div>


            <!-- More portfolio items -->
        </section>


        <section id="GH" class="about">
            <h1>AIxAI Grasshopper Plugin</h1>
            <p>Use Grasshopper as the user interface to employ the technologies mentioned above. This involves using a
                3D voxel sculpting tool within the Rhino environment, which intuitively changes the attributes in the
                voxel system.</p>

            <img src="assets\GrasshopperPlugin.png" alt="Project 2">

            <iframe src="https://www.youtube.com/embed/Io0Cp6axjHM?si=9GkqSlukKSBEY9cm&autoplay=1&loop=1"
                            frameborder="0" allow="autoplay; encrypted-media"
                            referrerpolicy="strict-origin-when-cross-origin">
                        </iframe>
        </section>

        <section id="about" class="about">
            <h2>About Me</h2>

            <p>Tianxiao Peng is a California licensed architect with 15 years of professional experience, specializing
                in innovative design using open source tools. He has extensive experience with Grasshopper scripting (12
                years) and Python programming (10 years) and is currently developing a Grasshopper plugin in C#.
                Tianxiao holds a Master of Science from SCI-Arc and a Master of Architecture degree from Ball State
                University. He is committed to leveraging technology's profound impact on architecture and society.</p>

        </section>

    </div>


    <footer class="contact">
        <h2>Contact Me</h2>
        <p>Email: pengtianxiao@gmail.com</p>
        <p>Phone: (+1) 626 688 0979</p>

    </footer>

    <script src="script.js"></script>
</body>

</html>